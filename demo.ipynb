{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.dataset as dataset\n",
    "import lib.params as params\n",
    "import lib.network as network\n",
    "import lib.path as path\n",
    "import lib.utils as utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with a learning rate of 1.0 for 1 epochs with batchs of size 3\n",
      "[construct_path_lists] parsing dir \"/out\" for files with \"data_\" in their name\n",
      "[construct_path_lists] parsing dir \"/out\" for files with \"labels_\" in their name\n"
     ]
    }
   ],
   "source": [
    "# read params\n",
    "with open(\"config/train.params\") as f:\n",
    "    learning_rate=float(params.read_param(f.readline()))\n",
    "    batch_size=int(params.read_param(f.readline()))\n",
    "    epoch=int(params.read_param(f.readline()))\n",
    "   \n",
    "    print(\"training with a learning rate of {} for {} epochs with batchs of size {}\".format(learning_rate,epoch,batch_size))\n",
    "\n",
    "data_all=np.load(path.construct_path_lists(\"out\",\"data_\")[-1])  \n",
    "label_all=np.load(path.construct_path_lists(\"out\",\"labels_\")[-1])  \n",
    "# print(data_all.shape,label_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init network\n",
      "WARNING:tensorflow:From /Users/micmelesse/Documents/thesis/lib/network.py:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "training\n",
      "epoch_000-batch_000: loss=nan\n",
      "epoch_000-batch_001: loss=nan\n",
      "epoch_000-batch_002: loss=nan\n",
      "epoch_000-batch_003: loss=nan\n",
      "epoch 0 took 862 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"init network\")\n",
    "net=network.R2N2(learning_rate)\n",
    "model_dir=\"out/model_{}_{}_{} {}\".format(learning_rate,epoch,batch_size,datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# train network\n",
    "loss_all=[]\n",
    "acc_all=[]\n",
    "N=len(data_all)\n",
    "print(\"training\")\n",
    "for e in range(epoch):\n",
    "    start_time=time.time()\n",
    "    perm=np.random.permutation(N)\n",
    "    data_all=data_all[perm]\n",
    "    label_all=label_all[perm]\n",
    "    split_size=math.ceil(N/batch_size)\n",
    "    data_batchs=np.array_split(data_all,split_size)\n",
    "    label_batchs=np.array_split(label_all,split_size)\n",
    "    \n",
    "    save_dir=\"{}/epoch_{:03d}\".format(model_dir,e)\n",
    "    if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "    loss_epoch=[]\n",
    "    acc_epoch=[]\n",
    "    batch_number=0\n",
    "    for data,label in zip(data_batchs,label_batchs):\n",
    "        fd={net.X:data, net.Y: label}\n",
    "        train_step=net.train_step(fd)\n",
    "        loss_epoch.append(net.batch_loss.eval(fd))\n",
    "        print(\"epoch_{:03d}-batch_{:03d}: loss={}\".format(e,batch_number,loss_epoch[-1]))\n",
    "        batch_number+=1\n",
    "        \n",
    "    loss_all.append(loss_epoch)\n",
    "    net.save(save_dir,\"loss\",loss_all)\n",
    "    print(\"epoch %d took %d seconds\"%(e,time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
